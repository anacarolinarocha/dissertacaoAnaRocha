\label{cap:TrabalhosRelacionados}%
\newcommand{\texCommand}[1]{\texttt{\textbackslash{#1}}}%

\newcommand{\exemplo}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}#1\end{minipage}}%
\\\vspace{\baselineskip}}%

\newcommand{\exemploVerbatim}[1]{%
\vspace{\baselineskip}%
\noindent\fbox{\begin{minipage}{\textwidth}%
#1\end{minipage}}%
\\\vspace{\baselineskip}}%

Esta revisão da literatura busca encontrar trabalhos onde se tenha abordado a classificação de textos. Buscou-se sobretudo estudos que envolveram a língua portuguesa no contexto de documentos jurídicos, embora alguma dessas premissas tenham sido relaxadas para alguns trabalhos citados.


%CLASIFICAÇÃO JURIDICA PORTUGUES
No trabalho realizado por um servidor do Tribunal Regional do Trabalho (TRT) da 2ª Região~\cite{ferauche}, extraiu-se informações relevantes dos textos das ementas jurisprudenciais e aplicou-se os algoritmos J4.8, Naïve Bayes (NB) e \textit{Sequential Minimal Optimization }(SMO) de forma individual e também combinados em um \textit{ensemble}, ou seja, um grupo de algoritmos que fazem uma votação da classe de cada documento, e vence a classe com maior número de votos. Foram escolhidas 10 categorias para análise, e utilizados 1000 documentos para a criação do modelo de cada categoria, sendo 500 documentos da categoria de interesse e os demais de outras categorias. Notou-se que o desempenho de cada algoritmo é bastante dependente da categoria, apresentando elevadas taxas de erro em determinadas categorias, e em outras não. Considerando a acurácia média dentre todas as categorias, tem-se que o J4.8 apresentou o valor 88,60\% e o ensemble 84,80\%. 

Outro trabalho que envolveu a categorização de documentos dentro da JT se encontra em~\cite{ticom_aplicacao_2007}. Neste estudo, analisou-se 104 sentenças e acórdãos que trataram de 4 assuntos diferentes. Foram removidas as palavras de pouca relevância semântica, também chamadas de \textit{stopwords}, e fez-se stemização nas palavras, onde busca-se remover a quantidade de palavras diferentes reduzindo-as aos seus radicais. Os modelos utilizados para a classificação foram criados partir de algoritmos de modelos lineares, NB e regras de decisão. O modelo linear apresentou os melhores resultados, com a métrica \textit{F-Measure} variando  entre 86,88\% e 95,82\% entre os diferentes assuntos. Assim, embora o contexto negocial seja bastante similar ao apresentado nestes dois estudos, o presente trabalho se diferencia por abordar uma quantidade de documentos mais próxima à quantidade real, bem como por abranger um maior número de categorias possíveis.


%CLASIFICAÇÃO JURIDICA
Em~\cite{chinese}, os autores fizeram uso de técnicas de mineração de texto para classificarem documentos jurídicos chinêses. Fez-se uso de um conjunto de 6735 textos jurídicos previamente classificados em 13 categorias. Destes textos, extraiu-se apenas a seção que é interessante para o objetivo da classificação, e depois aplicou-se técnicas de pré-processamento de texto. Importante notar que além da lista de \textit{stopwords} chinesa comum, os autores construíram uma lista de \textit{stopwords} jurídicas, que eram muito frequentes e pouco relevantes para a tarefa de classificação. As palavras do texto  foram representadas por meio e vetores TF-IDF, onde se representa a relevância de cada palavra perante o conjunto de documentos, e para a redução da dimensionalidade  aplicou-se os métodos Princial Component Analysis (PCA) e uma variação do Singular Vector Decomposition (SVD). Utilizou-se os algoritmos NB, Árvores de Decisão, \textit{Random Forest} e \textit{Support Vector Machine }(SVM) para a tarefa de classificação, e eles foram comparados por meio das métricas de acurácia, precisão, \textit{recall} e \textit{F-Measure}. O melhor resultado foi do SVM, com métrica \textit{F-Measure} de 87,00\%.

Em~\cite{european_court},tentou-se predizer a decisão de um juiz para um determinado processo na Corte Europeia de Direitos Humanos. Criou-se um modelo de classificação binária, onde a entrada é o texto com a problemática do processo e o resultado esperado é a suposta decisão do juiz. Este trabalho usou documentos escritos em inglês, e fez uso de técnicas de processamento de linguagem natural, n-gramas e tópicos para montar o modelo preditivo baseado no SVM. Ao final do trabalho executado, chegou-se a um modelo com de acurácia de 79,00\% (valor considerado forte neste tipo de caso).


%CLASSIFICACAO JURIDICA MULTICLASSE PORTUGUES
%\cite{goncalves_is_2005} %ja escrevi sobre

%CLASSIFICACAO JURIDICA MULTICLASSE
%\cite{de_maat_machine_2010} 
%\cite{chinese} %ja escrevi sobre

%CLASSIFICACAO JURIDICA MULTILABEL PORTUGUES

%CLASSIFICACAO JURIDICA MULTILABEL
%\cite{mencia_efficient_2008}

%ADICIONOU STOPWORD
%\cite{ladeirahuffman}

%CLASSIFICAÇÃO DE TEXTO GERAL MULTILABEL
%\cite{ladeirahuffman}



%CLASSIFICAÇÃO DE TEXTO

No trabalho ~\cite{tripadivisor}, criou-se um modelo de classificação para as revisões dos usuários sobre estabelecimentos listados no sítio TripAdvisor \footnote{\url{https://www.tripadvisor.com/}}. Foi abordado o problema de se ter uma base de dados com registros previamente categorizados, onde há erro de cadastro das categorias. 
Para reduzir a quantidade de dados errados no conjunto de treinamento, buscou-se encontrar um centroide baseado nos vetores TD-IDF dos documentos que representasse cada uma das classes, e escolheu-se apenas os elementos mais próximos do centroide para compor um conjunto confiável para treinamento, o que elevou a métrica \textit{F-Measure } de 77,00\% para 90,00\%. O algoritmo utilizado foi o NB Multinomial, \textit{Stochastic Gradient Descent Classifier} (SGDClassifier), \textit{Random Forest}, \textit{Ada Boost}, \textit{Linear Support Vector Classification} (\textit{Linear} SVC) e Regressão Logística, sendo que o NB apresentou os melhores resultados. Maiores informações sobre a redução de ruídos em um conjunto de dados podem ser encontradas na pesquisa consolidada no estudo~\cite{mislabeled_survey} e em outros trabalhos relacionados ~\cite{liu_preprocessing_2017,svmop}.



Outro estudo relevante abordou o problema de se ter uma elevada quantidade de categorias possíveis dentro de um problema de classificação. No estudo realizado em ~\cite{oliveira_intelligent_nodate} busca-se a categorização da atividade econômica de empresas de acordo com o código CNAE . Neste trabalho, utilizou-se 3281 descrições com aproximadamente 70 palavras cada, com o objetivo de encontrar qual dos 765 códigos da tabela CNAE representa a atividade econômica da empresa. Foram criados dois modelos, um do modelo de espaço vetorial considerando a distância do cosseno, semelhante ao KNN, e outro com uma rede neural (\textit{Virtual  Generalizing RAM Weightless Neural Network}). 
No primeiro obteve-se acurácia de 63,36\% e no segundo 67,03\%. 

%OUTROS TRABALHOS QUE FALARAM DESTE PROBLEMA DA ELEVADA QUANTIDADE DE CATEGORIAS POSSÍVEIS....
%\cite{mencia_efficient_2008}

%\myworries{//TODO: buscar resultados com word embeddings}

%CLASSIFICAÇÃO DE TEXTO / REDUÇÃO DA DIMENSIONALIDADE

Alguns são os trabalhos que investigaram diferentes formas de redução da dimensionalidade dos vetores de texto. Em~\cite{goncalves_is_2005}, apresentou-se um problema de classificação multirrótulo com 801 rótulos possíveis. O conjunto de dados era composto de 8151 documentos da Procuradoria Geral de Portugal. Aplicando-se apenas o algoritmo SVM, variou-se a forma como foram filtradas e selecionas as palavras, e analisou-se o impacto da utilização de técnicas de análise sintática do texto. Verificou-se que é possível reduzir significativamente a dimensão dos vetores escolhendo um conjunto de classes sintáticas sem que haja degradação das métricas, conseguindo-se otimização no tempo de execução. 

Outro trabalho que aborda este tema se encontra em~\cite{ciarelli_agglomeration_2009}. Neste trabalho, analisa-se diferentes formas de redução da dimensionalidade, abordando métodos que analisam apenas a frequência das palavras e outros que tentam agrupar palavras. Foram utilizados 4 datasets diferentes, cuja quantidade de palavras variou entre 856 e 3000, e a quantidade de categorias variou entre 9 e 52. Para a classificação, utilizou-se o KNN. Analisando-se o conjunto de dados com maior número de categorias, avaliando-se o custo-benefício de cada algoritmos, tem-se que o método de Informação Mútua (MI) (considerando a probabilidade de cada palavra) e \textit{Latent Semantic Indexing }(LSI) apresentam os melhores resultados a partir de dimensões de tamanho 100.

%OUTROS ALGORITMOS
Considerando a dificuldade do trabalho apresentado, uma vez que trabalha com termos específicos do jargão jurídico, documentos com elevada quantidade de informação e um elevado número de categorias, vale citar trabalhos que tiveram resultados de baixo desempenho com os algoritmos convencionais. Nesse sentido, pode-se citar \cite{ladeirahuffman}, onde buscou-se classificar denúncias realizadas na Controladoria Geral da União. Foram testados os algorítimos SVM, Random Forest, Naïve Bayes e Árvore de decisão (C4.5). Esses algoritmos não se mostraram escaláveis à medida que se aumentava a quantidade de classes, e apresentaram desempenho abaixo do esperado, chegando a um máximo de 59,00\% de precisão com o SVM. Assim, resolveu-se abordar o problema como uma classificação multirrótulo, aumentando as chances de acerto do classificador. Buscando-se resolver a questão da escalabilidade, o modelo de classificação foi criado com uma combinação da Codificação Adaptativa de Huffman com a Descrição de Comprimento Mínimo (CAH+MDL). Mostrou-se que esse algorítimo apresentou taxa de revocação maior que o SVM, mas uma precisão menor. Com a abordagem multirrótulo e o CAH-MDL, somados a um pré-processamento de texto, que proporcionou maior redução da dimensionalidade, conseguiu-se uma precisão de 84,00\%.

%CONCLUSAO
De maneira geral, dentre os trabalhos encontrados, nota-se que o desempenho dos algoritmos varia em cada classe \cite{de_maat_machine_2010,goncalves_is_2005}, ou seja, para um mesmo conjunto de documentos, a taxa de erro é pequena em algumas classes e grande em outras, seja por uma dificuldade inerente ao contexto, ou por haver um conjunto de dados desbalanceado. 
Dentre os algoritmos utilizados, o que apresentou melhor desempenho foi o SVM. Obtiveram-se bons resultados aplicando técnicas de redução de dimensionalidade.
